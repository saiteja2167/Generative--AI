# Task 1: Find the value of x at which f(x) = 5x^4 + 3x^2 + 10 has minimum value

def gradient_descent_1d(alpha=0.01, epsilon=1e-6, max_iter=10000):
    def f_prime(x):
        return 20 * x**3 + 6 * x  # Derivative of f(x)

    x = 0.0  # Initial guess
    for _ in range(max_iter):
        grad = f_prime(x)
        if abs(grad) < epsilon:
            break
        x -= alpha * grad
    return x

x_min = gradient_descent_1d()
print(f"Minimum value of f(x) occurs at x = {x_min}")

# Task 2: Find the values of x and y at which g(x, y) = 3x^2 + 5e^(-y) + 10 has minimum value

def gradient_descent_2d(alpha=0.01, epsilon=1e-6, max_iter=10000):
    def f_prime_x(x):
        return 6 * x  # Partial derivative with respect to x

    def f_prime_y(y):
        return -5 * (2.718281828459045 ** (-y))  # Partial derivative with respect to y

    x, y = 0.0, 0.0  # Initial guesses
    for _ in range(max_iter):
        grad_x = f_prime_x(x)
        grad_y = f_prime_y(y)
        if abs(grad_x) < epsilon and abs(grad_y) < epsilon:
            break
        x -= alpha * grad_x
        y -= alpha * grad_y
    return x, y

x_min, y_min = gradient_descent_2d()
print(f"Minimum value of g(x, y) occurs at x = {x_min}, y = {y_min}")

# Task 3: Find the value of x at which z(x) = 1 / (1 + e^(-x)) has minimum value

def gradient_descent_sigmoid(alpha=0.01, epsilon=1e-6, max_iter=10000):
    def z_prime(x):
        exp_neg_x = 2.718281828459045 ** (-x)
        return -exp_neg_x / ((1 + exp_neg_x) ** 2)  # Derivative of sigmoid function

    x = 0.0  # Initial guess
    for _ in range(max_iter):
        grad = z_prime(x)
        if abs(grad) < epsilon:
            break
        x -= alpha * grad
    return x

x_min_sigmoid = gradient_descent_sigmoid()
print(f"Minimum value of z(x) occurs at x = {x_min_sigmoid}")

# Task 4: Find the optimal values of M and C for minimizing SE = (ExpectedOutput - PredictedOutput)^2

def gradient_descent_linear(alpha=0.01, epsilon=1e-6, max_iter=10000):
    expected_outputs = [1, 2, 3, 4, 5]  # Example expected outputs
    inputs = [1, 2, 3, 4, 5]  # Example inputs

    def predicted_output(m, c, x):
        return m * x + c

    def se_grad_m(m, c):
        return -2 * sum((e - predicted_output(m, c, x)) * x for e, x in zip(expected_outputs, inputs))

    def se_grad_c(m, c):
        return -2 * sum(e - predicted_output(m, c, x) for e, x in zip(expected_outputs, inputs))

    m, c = 0.0, 0.0  # Initial guesses
    for _ in range(max_iter):
        grad_m = se_grad_m(m, c)
        grad_c = se_grad_c(m, c)
        if abs(grad_m) < epsilon and abs(grad_c) < epsilon:
            break
        m -= alpha * grad_m
        c -= alpha * grad_c
    return m, c

m_opt, c_opt = gradient_descent_linear()
print(f"Optimal values are M = {m_opt}, C = {c_opt}")
